{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random seeds for deterministic results\n",
    "seed = 4242\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  \\\n",
      "0           0  May 30 (Reuters) - ANZ Bank New Zealand on Wed...   \n",
      "1           1  SYDNEY (Reuters) - Australiaâ€™s Westpac Banking...   \n",
      "2           2  SAN ANTONIO--(BUSINESS WIRE)-- Clear Channel O...   \n",
      "3           3  SAN ANTONIO--(BUSINESS WIRE)-- Clear Channel O...   \n",
      "4           4  May 3 (Reuters) - Pembina Pipeline Corp:\\n* OR...   \n",
      "\n",
      "   change_+1day  change_+1week  change_+1month  change_+3month  \\\n",
      "0     -2.279028       0.928909       -1.944386        9.508421   \n",
      "1     -0.698973      -1.491145        1.584344       -6.290767   \n",
      "2     -1.111115      -2.222220        3.333335        4.444440   \n",
      "3     -1.111115      -2.222220        3.333335        4.444440   \n",
      "4      0.000000       2.091936        1.532117        7.807901   \n",
      "\n",
      "   change_+6month  type_1_positive_change_+1day  \\\n",
      "0       22.969072                             0   \n",
      "1      -11.136997                             0   \n",
      "2       30.222225                             0   \n",
      "3       30.222225                             0   \n",
      "4       -2.563344                             0   \n",
      "\n",
      "   type_1_positive_change_+1week  type_1_positive_change_+1month  \\\n",
      "0                              1                               0   \n",
      "1                              0                               1   \n",
      "2                              0                               1   \n",
      "3                              0                               1   \n",
      "4                              1                               1   \n",
      "\n",
      "   type_1_positive_change_+3month  type_1_positive_change_+6month  \n",
      "0                               1                               1  \n",
      "1                               0                               0  \n",
      "2                               1                               1  \n",
      "3                               1                               1  \n",
      "4                               1                               0  \n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "csv = pd.read_csv('full_text_w_labels1.csv')\n",
    "print(csv.head(5))\n",
    "texts = csv['text'].values\n",
    "labels = csv['type_1_positive_change_+1day'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples: 55035\n",
      "labels: 55035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc6d6a5ef28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('examples:', len(texts))\n",
    "print('labels:', len(labels))\n",
    "\n",
    "sns.histplot(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(labels, return_dict=True, first_class_index=0):\n",
    "    if isinstance(labels[0], list) or isinstance(labels[0], np.ndarray):\n",
    "        labels = [y.argmax() for y in labels]\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "    if return_dict:\n",
    "        weight_dict = {}\n",
    "        for key in range(len(class_weights)):\n",
    "            weight_dict[key + first_class_index] = class_weights[key]\n",
    "        return weight_dict\n",
    "    else:\n",
    "        return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data restrictions\n",
    "vocabulary = 10000\n",
    "length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer texts (filters for punctuation)\n",
    "tokenizer = Tokenizer(num_words=vocabulary)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "#if save_model:\n",
    "#    torch.save(tokenizer, f'models/{model_name}.tokenizer')\n",
    "\n",
    "# tansform texts to padded sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "sequences = pad_sequences(sequences, maxlen=length, truncating='post', padding='pre')\n",
    "\n",
    "# compute balanced class weights if needed\n",
    "class_weights = calculate_class_weights(labels=labels)\n",
    "\n",
    "# train / test split and shuffle\n",
    "x_train, x_test, y_train, y_test = train_test_split(sequences, labels, random_state=seed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayDataSet(Dataset):\n",
    "    def __init__(self, x, y, t):\n",
    "        self.X = torch.tensor(x, dtype=t)\n",
    "        self.Y = torch.tensor(y, dtype=t)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index], self.Y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# setup data loaders for training and evaluation\n",
    "train_loader = DataLoader(dataset=ArrayDataSet(x=x_train, y=y_train, t=torch.long),\n",
    "                          batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(dataset=ArrayDataSet(x=x_test, y=y_test, t=torch.long),\n",
    "                          batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 200\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocabulary, embedding_dim=features)\n",
    "        self.convolutions = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5, features)),\n",
    "                                           nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(15, features)),\n",
    "                                           nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(30, features)),\n",
    "                                           nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(50, features))])\n",
    "        #self.fc1 = nn.Linear(in_features=32 * len(self.convolutions), out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32 * len(self.convolutions), out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.unsqueeze(x, dim=1)\n",
    "        xs = []\n",
    "        for convolution in self.convolutions:\n",
    "            c = F.softplus(convolution(x))\n",
    "            c = torch.squeeze(c)\n",
    "            c = F.max_pool1d(c, kernel_size=c.size()[2])\n",
    "            xs.append(c)\n",
    "        x = torch.cat(xs, dim=2)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        #x = F.leaky_relu(self.fc1(x), negative_slope=0.1)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocabulary, embedding_dim=features)\n",
    "        self.lstm_1 = nn.LSTM(input_size=features, hidden_size=32, num_layers=3, dropout=0.125,\n",
    "                              bidirectional=True, batch_first=True)\n",
    "        # self.lstm_2 = nn.LSTM(input_size=32 * length, hidden_size=32, num_layers=1, dropout=0.125,\n",
    "        #                       bidirectional=True, batch_first=True)\n",
    "        #self.fc1 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        # print(x.shape)\n",
    "        x, _ = self.lstm_1(x)\n",
    "        # print(x.shape)\n",
    "        x = x[:, -1, :]\n",
    "        # h = torch.cat(h, dim=1)\n",
    "        # print('h:', h.shape)\n",
    "        # print(x.shape)\n",
    "        # x = x.reshape(x.shape[0], -1)\n",
    "        # print(x.shape)\n",
    "        # x, h = self.lstm_2(x)\n",
    "        # print(x.shape)\n",
    "        #x = self.fc1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' # change to cpu for cpu and cuda for gpu training\n",
    "net = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(batch_size, length).long().to(device)\n",
    "p = net(t)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)\n",
    "#optimizer = optim.RMSprop(net.parameters(), lr=0.001, alpha=0.9, eps=1e-07)\n",
    "class_weights = torch.tensor(list(class_weights.values())).type(torch.FloatTensor).cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(class_weights)\n",
    "#loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch in enumerate(valid_loader):\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # make predictions\n",
    "            predictions = net(x_batch)\n",
    "            # get loss\n",
    "            loss = loss_function(predictions, y_batch)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # count correct predictions\n",
    "            predicted_classes = torch.max(predictions.data, dim=1)[1]\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted_classes == y_batch).sum().item()\n",
    "\n",
    "    net.train()\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:   1] train_loss:   0.003 - train_acc:   59.63% | val_loss:   0.002 - val_acc:   63.87%\n",
      "[Epoch:   2] train_loss:   0.002 - train_acc:   82.05% | val_loss:   0.002 - val_acc:   69.99%\n",
      "[Epoch:   3] train_loss:   0.001 - train_acc:   91.60% | val_loss:   0.003 - val_acc:   72.20%\n",
      "[Epoch:   4] train_loss:   0.001 - train_acc:   94.56% | val_loss:   0.003 - val_acc:   72.64%\n",
      "[Epoch:   5] train_loss:   0.001 - train_acc:   95.54% | val_loss:   0.003 - val_acc:   72.45%\n",
      "[Epoch:   6] train_loss:   0.001 - train_acc:   95.99% | val_loss:   0.003 - val_acc:   72.32%\n",
      "[Epoch:   7] train_loss:   0.001 - train_acc:   96.27% | val_loss:   0.003 - val_acc:   73.83%\n",
      "[Epoch:   8] train_loss:   0.001 - train_acc:   96.47% | val_loss:   0.003 - val_acc:   72.60%\n",
      "[Epoch:   9] train_loss:   0.001 - train_acc:   96.58% | val_loss:   0.003 - val_acc:   72.09%\n",
      "[Epoch:  10] 1.74%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-6c70155bd099>\", line 27, in <module>\n",
      "    loss.backward()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 221, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 132, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    from ._api.v2 import __operators__\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.compat'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "accuracies = []\n",
    "losses = []\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        sys.stdout.write(f'\\r[Epoch: {epoch + 1:3d}] {batch_index / len(train_loader) * 100:4.2f}%')\n",
    "        sys.stdout.flush()\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # make predictions\n",
    "        predictions = net(x_batch)\n",
    "        # get loss\n",
    "        loss = loss_function(predictions, y_batch)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # calculate gradients using the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # perform optimizer step on the network parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # count correct predictions\n",
    "        predicted_classes = torch.max(predictions.data, dim=1)[1]\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted_classes == y_batch).sum().item()\n",
    "\n",
    "    epoch_loss, epoch_acc = evaluate()\n",
    "    result = f'[Epoch: {epoch + 1:3d}] train_loss: {running_loss / total:7.3f}'\n",
    "    result += f' - train_acc: {100 * correct / total:7.2f}% |'\n",
    "    result += f' val_loss: {epoch_loss:7.3f} - val_acc: {100 * epoch_acc:7.2f}%'\n",
    "    sys.stdout.write(f'\\r{result}\\n')\n",
    "\n",
    "    accuracies.append(correct / total)\n",
    "    losses.append(running_loss / total)\n",
    "    val_accuracies.append(epoch_acc)\n",
    "    val_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(20), accuracies, label='train_acc')\n",
    "plt.plot(range(20), val_accuracies, label='val_acc')\n",
    "plt.ylim((0, 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(20), losses, label='train_loss')\n",
    "plt.plot(range(20), val_losses, label='val_loss')\n",
    "plt.ylim((0, 0.03))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(class_labels, device):\n",
    "    n_classes = len(class_labels)\n",
    "    confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, classes) in enumerate(valid_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            classes = classes.to(device)\n",
    "            outputs = net(inputs)\n",
    "            predictions = torch.max(outputs, dim=1)[1]\n",
    "            for t, p in zip(classes.view(-1), predictions.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    net.train()\n",
    "    confusion_matrix = confusion_matrix.numpy()\n",
    "    cm_df = pd.DataFrame(confusion_matrix, class_labels, class_labels)\n",
    "    sns.set(font_scale=1.1)\n",
    "    ax = sns.heatmap(cm_df, cmap='Oranges', annot=True, annot_kws={'size': 11}, cbar=False, fmt='g')\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(class_labels=['0', '1'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
